{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: End-to-End Synthetic Data Generation Flow\n",
    "\n",
    "In this exercise, we will build a comprehensive synthetic dataset for a **Service Desk** use case entirely from scratch using NVIDIA Data Designer. This hands-on session covers the full lifecycle of synthetic data generationâ€”from defining the schema to evaluating quality.\n",
    "\n",
    "### **Learning Objectives**\n",
    "By the end of this exercise, you will know how to:\n",
    "\n",
    "1.  **Define Structural Data**: Use deterministic **Samplers** to create structured fields like IDs, scores, names, and categories.\n",
    "2.  **Generate Realistic Text**: Leverage **LLMs** to generate context-aware ticket descriptions, agent resolutions, and customer feedback.\n",
    "3.  **Implement Business Logic**: Use **Expression Columns** to derive new data points based on generated values (e.g., flagging low CSAT scores).\n",
    "4.  **Evaluate Quality**: Set up an **LLM-as-a-Judge** to automatically score the quality (Professionalism, Relevance) of generated resolutions.\n",
    "5.  **Analyze Results**: Generate, export, and profile the final dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup\n",
    "Import necessary classes from `data_designer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_designer.essentials import (\n",
    "    DataDesigner,\n",
    "    ModelConfig,\n",
    "    InferenceParameters,\n",
    "    CategorySamplerParams,\n",
    "    DataDesignerConfigBuilder,\n",
    "    ExpressionColumnConfig,\n",
    "    JudgeScoreProfilerConfig,\n",
    "    LLMJudgeColumnConfig,\n",
    "    LLMTextColumnConfig,\n",
    "    PersonFromFakerSamplerParams,\n",
    "    Score,\n",
    "    SamplerColumnConfig,\n",
    "    SamplerType,\n",
    "    SubcategorySamplerParams,\n",
    "    UniformSamplerParams,\n",
    "    UUIDSamplerParams,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_designer.config.models import ModelProvider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure Model Provider\n",
    "Set up the connection to the Local NIM (NVIDIA Inference Microservice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_nim_provider = ModelProvider(\n",
    "    name=\"local-nim\",\n",
    "    endpoint=\"http://localhost:8080/v1\",\n",
    "    provider_type=\"openai\",\n",
    "    api_key=\"dummy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_designer = DataDesigner(model_providers=[local_nim_provider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"nvidia/nvidia-nemotron-nano-9b-v2\" \n",
    "MODEL_PROVIDER = \"local-nim\"\n",
    "MODEL_ALIAS = \"local-model\"\n",
    "SYSTEM_PROMPT = \"/no_think\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Model Configuration\n",
    "Specify the model to be used for generation (e.g., `nvidia-nemotron-nano-9b-v2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = [\n",
    "    ModelConfig(\n",
    "        alias=MODEL_ALIAS,\n",
    "        model=MODEL_ID,\n",
    "        provider=MODEL_PROVIDER,\n",
    "        inference_parameters=InferenceParameters(\n",
    "            temperature=0.5,\n",
    "            top_p=1.0,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialize Config Builder\n",
    "Start building the data generation configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder = DataDesignerConfigBuilder(model_configs=model_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define Base Columns (Samplers)\n",
    "We start by defining the structural columns of our dataset using deterministic samplers.\n",
    "- `ticket_id`: Unique identifier.\n",
    "- `priority_score`: Random score between 1-10.\n",
    "- `employee`: Fake persona.\n",
    "- `department`: Categorical selection.\n",
    "- `issue_type`: Dependent on department (Subcategory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"ticket_id\",\n",
    "        sampler_type=SamplerType.UUID,\n",
    "        params=UUIDSamplerParams(prefix=\"INC-\", short_form=True)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"priority_score\",\n",
    "        sampler_type=SamplerType.UNIFORM,\n",
    "        params=UniformSamplerParams(\n",
    "            low=1.0,\n",
    "            high=10.0,\n",
    "            decimal_places=2  # Round to 2 decimal places\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"employee\",\n",
    "        sampler_type=SamplerType.PERSON_FROM_FAKER,\n",
    "        params=PersonFromFakerSamplerParams(\n",
    "            locale=\"en_US\",\n",
    "            age_range=[22, 65]\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"department\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(\n",
    "            values=[\"IT Support\", \"HR\", \"Facilities\"],\n",
    "            weights=[0.5, 0.3, 0.2]  # IT Support is most common\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"issue_type\",\n",
    "        sampler_type=SamplerType.SUBCATEGORY,\n",
    "        params=SubcategorySamplerParams(\n",
    "            category=\"department\",  # Links to the column above\n",
    "            values={\n",
    "                \"IT Support\": [\"Login Failure\", \"Hardware Malfunction\", \"Software Installation\", \"VPN Issue\"],\n",
    "                \"HR\": [\"Benefits Inquiry\", \"Payroll Discrepancy\", \"Conflict Resolution\"],\n",
    "                \"Facilities\": [\"Desk Repair\", \"Temperature Control\", \"Access Badge\"],\n",
    "            },\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate Text Content (LLM Columns)\n",
    "Now we use the LLM to generate realistic text content based on the sampled values.\n",
    "- `ticket_description`: Generated based on employee, department, and issue type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are {{ employee.first_name }} {{ employee.last_name }}, an employee in the {{ department }} department.\n",
    "You are submitting a helpdesk ticket regarding: {{ issue_type }}.\n",
    "Your priority score is {{ priority_score }}/10.\n",
    "\n",
    "Write a short description of your issue.\n",
    "If priority is high (>8), sound urgent and frustrated.\n",
    "If priority is low (<4), sound casual.\n",
    "Include specific details relevant to {{ issue_type }}.\n",
    "\n",
    "Output ONLY the ticket description.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    LLMTextColumnConfig(\n",
    "        name=\"ticket_description\",\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        model_alias=MODEL_ALIAS,\n",
    "        prompt=PROMPT_TEMPLATE,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `agent_resolution`: Generated based on the ticket description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION_PROMPT = \"\"\"\n",
    "You are an expert Helpdesk Agent.\n",
    "Ticket: {{ ticket_description }}\n",
    "Issue Type: {{ issue_type }}\n",
    "\n",
    "Write a brief, professional resolution note explaining how you solved this issue.\n",
    "Be specific to the technical details mentioned in the ticket.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    LLMTextColumnConfig(\n",
    "        name=\"agent_resolution\",\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        model_alias=MODEL_ALIAS,\n",
    "        prompt=RESOLUTION_PROMPT,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `csat_score`: Simulated customer satisfaction score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSAT_PROMPT = \"\"\"\n",
    "You are {{ employee.first_name }}.\n",
    "You submitted a ticket: \"{{ ticket_description }}\"\n",
    "The agent resolved it with: \"{{ agent_resolution }}\"\n",
    "\n",
    "Rate your satisfaction with this resolution on a scale of 1 to 5 (5 being best).\n",
    "Consider:\n",
    "- Did they address your specific details?\n",
    "- Was the tone appropriate?\n",
    "\n",
    "Output ONLY the number (1, 2, 3, 4, or 5).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    LLMTextColumnConfig(\n",
    "        name=\"csat_score\",\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        model_alias=MODEL_ALIAS,\n",
    "        prompt=CSAT_PROMPT\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Derived Columns (Expressions)\n",
    "Create columns based on logical expressions derived from other columns.\n",
    "- `follow_up_required`: Logic to flag low CSAT scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    ExpressionColumnConfig(\n",
    "        name=\"follow_up_required\",\n",
    "        expr=\"{% if csat_score | int < 3 %}YES{% else %}NO{% endif %}\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Quality Evaluation (LLM Judge)\n",
    "Add an LLM-as-a-Judge to evaluate the quality of the generated agent resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_column(\n",
    "    LLMJudgeColumnConfig(\n",
    "        name=\"quality_evaluation\",\n",
    "        model_alias=MODEL_ALIAS,\n",
    "        prompt=\"Evaluate the quality of the following agent resolution:\\n\\nResolution: {{ agent_resolution }}\\nTicket: {{ ticket_description }}\",\n",
    "        scores=[\n",
    "            Score(\n",
    "                name=\"Professionalism\",\n",
    "                description=\"Evaluate the professional tone of the response.\",\n",
    "                options={1: \"Rude\", 3: \"Neutral\", 5: \"Highly Professional\"},\n",
    "            ),\n",
    "            Score(\n",
    "                name=\"Relevance\",\n",
    "                description=\"Does the resolution directly address the issue described in the ticket?\",\n",
    "                options={1: \"Irrelevant\", 3: \"Somewhat Relevant\", 5: \"Highly Relevant\"},\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.add_profiler(\n",
    "    JudgeScoreProfilerConfig(model_alias=MODEL_ALIAS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Preview Generation\n",
    "Generate a small sample (5 records) to verify the configuration and quality before running the full job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = data_designer.preview(config_builder, num_records=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview.dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview.analysis.to_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Full Dataset Generation\n",
    "Generate the complete dataset (10 records for this demo) and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_designer.create(config_builder, num_records=10, dataset_name='ServiceDesk-SDG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = result.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Analysis\n",
    "Review the generated data statistics and judge scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = result.load_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.to_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
