{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing Your Own LangGraph Agent to NeMo Agent Toolkit\n",
    "\n",
    "In this notebook, we'll show you how to integrate an existing LangGraph agent with the NeMo Agent Toolkit (NAT).\n",
    "\n",
    "You'll learn how to wrap LangGraph agents so they work smoothly with NAT. This lets you take advantage of NAT features like MCP compatibility, observability, optimization, and profiling in your existing LangGraph agent systems without refactoring your existing code.\n",
    "\n",
    "**Key Difference**: Unlike traditional LangChain agents that use the `AgentExecutor` pattern, LangGraph uses a **graph-based architecture** with nodes and edges, providing more flexibility and control over agent execution flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [0.0) Setup](#setup)\n",
    "  - [0.1) Prerequisites](#prereqs)\n",
    "  - [0.2) API Keys](#api-keys)\n",
    "  - [0.3) Installing NeMo Agent Toolkit](#installing-nat)\n",
    "- [1.0) Defining an 'Existing' LangGraph Agent](#defining-existing-agent)\n",
    "- [2.0) Existing Agent Migration](#migration)\n",
    "  - [2.1) Migration Part 1: Transforming Your Existing Agent into a Workflow](#migration-part-1)\n",
    "  - [2.2) Migration Part 2: Making Your Agent Configurable](#migration-part-2)\n",
    "  - [2.3) Migration Part 3: Integration with NeMo Agent Toolkit](#migration-part-3)\n",
    "  - [2.4) Migration Part 4: A Zero-Code Configuration](#migration-part-4)\n",
    "- [3) Next Steps](#next-steps)\n",
    "\n",
    "<span style=\"color:rgb(0, 31, 153); font-style: italic;\">Note: In Google Colab use the Table of Contents tab to navigate.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "# 0.0) Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prereqs\"></a>\n",
    "## 0.1) Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Platform:** Linux, macOS, or Windows\n",
    "- **Python:** version 3.11, 3.12, or 3.13\n",
    "- **Python Packages:** `pip`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"api-keys\"></a>\n",
    "## 0.2) API Keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, you will need the following API keys to run all examples end-to-end:\n",
    "\n",
    "- **NVIDIA Build:** You can obtain an NVIDIA Build API Key by creating an [NVIDIA Build](https://build.nvidia.com) account and generating a key at https://build.nvidia.com/settings/api-keys\n",
    "- **Tavily:** You can obtain a Tavily API Key by creating a [Tavily](https://www.tavily.com/) account and generating a key at https://app.tavily.com/home\n",
    "\n",
    "Then you can run the cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"NVIDIA_API_KEY\" not in os.environ:\n",
    "    nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    tavily_api_key = getpass.getpass(\"Enter your Tavily API key: \")\n",
    "    os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"installing-nat\"></a>\n",
    "## 0.3) Installing NeMo Agent Toolkit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended way to install NAT is through `pip` or `uv pip`.\n",
    "\n",
    "First, we will install `uv` which offers parallel downloads and faster dependency resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /home/ubuntu/.venv/lib/python3.12/site-packages (0.9.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install uv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeMo Agent toolkit can be installed through the PyPI `nvidia-nat` package.\n",
    "\n",
    "There are several optional subpackages available for NAT. The `langchain` subpackage contains useful components for integrating and running with [LangChain](https://python.langchain.com/docs/introduction/) and [LangGraph](https://langchain-ai.github.io/langgraph/).\n",
    "\n",
    "**Note**: LangGraph is part of the LangChain ecosystem and is included when you install `nvidia-nat[langchain]`. This single installation provides both LangChain and LangGraph dependencies.\n",
    "\n",
    "Since LangGraph will be used later in this notebook, let's install NAT with the optional `langchain` subpackage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-nat[langchain] is already installed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "uv pip show -q \"nvidia-nat-langchain\"\n",
    "if [ $? -ne 0 ]; then\n",
    "    uv pip install \"nvidia-nat[langchain]\"\n",
    "else\n",
    "    echo \"nvidia-nat[langchain] is already installed\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that both LangChain and LangGraph are available:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"defining-existing-agent\"></a>\n",
    "# 1.0) Defining an 'Existing' LangGraph Agent\n",
    "\n",
    "In this case study, we will use a simple, self-contained LangGraph agent as a proxy for your 'existing' agent. This agent comes equipped with a search tool that is capable of retrieving context from the internet using the Tavily API.\n",
    "\n",
    "**Key Difference from LangChain**: Unlike traditional LangChain agents that use the `AgentExecutor` pattern, LangGraph uses a **graph-based architecture** with nodes and edges. This provides more flexibility and control over the agent's execution flow.\n",
    "\n",
    "The cell below defines a simple LangGraph agent with a string input query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "def existing_agent_main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python langgraph_agent.py \\\"Your question here\\\"\")\n",
    "        sys.exit(1)\n",
    "    user_input = sys.argv[1]\n",
    "\n",
    "    # Initialize a tool to search the web\n",
    "    search = TavilySearch(\n",
    "        max_results=5,\n",
    "        api_key=os.getenv(\"TAVILY_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Initialize a LLM client\n",
    "    llm = ChatNVIDIA(\n",
    "        model=\"meta/llama-3.3-70b-instruct\",\n",
    "        temperature=0.2,\n",
    "        max_tokens=2048,\n",
    "        api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Create tools list\n",
    "    tools = [search]\n",
    "\n",
    "    # Create a LangGraph ReAct agent using the prebuilt function\n",
    "    # This creates a StateGraph with agent and tool nodes automatically\n",
    "    graph = create_react_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    # Invoke the agent with a user query\n",
    "    # LangGraph uses message-based state\n",
    "    response = graph.invoke({\"messages\": [(\"user\", user_input)]})\n",
    "\n",
    "    # Extract and print the final response\n",
    "    final_message = response[\"messages\"][-1]\n",
    "    print(final_message.content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    existing_agent_main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main components to this LangGraph agent:\n",
    "\n",
    "* **a web search tool (Tavily)** - for retrieving information from the internet\n",
    "\n",
    "* **an LLM (Llama 3.3)** - for reasoning and generating responses\n",
    "\n",
    "* **a graph-based agent system (LangGraph's `create_react_agent`)** - for orchestrating the agent's execution\n",
    "\n",
    "The agent is constructed using LangGraph's `create_react_agent` function, which automatically creates a state graph with:\n",
    "- An **agent node** that calls the LLM\n",
    "- **Tool nodes** for executing tools\n",
    "- **Conditional edges** for routing between agent and tools\n",
    "\n",
    "We pass the requested input into the graph and get a response back through the message state.\n",
    "\n",
    "All of the components in use come from LangGraph/LangChain, but any other framework or example could also work.\n",
    "\n",
    "Next we will run this sample agent to validate that it works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on LangGraph vs LangChain Parameters**: \n",
    "\n",
    "We're using the same parameters as the LangChain example (`max_results=2`, `temperature=0.0`, `max_completion_tokens=1024`) for consistency. However, you may notice that LangGraph's `create_react_agent` sometimes produces different quality responses compared to LangChain's `AgentExecutor` with identical settings.\n",
    "\n",
    "This is because:\n",
    "- **Different default system prompts** between the frameworks\n",
    "- **Different agent execution patterns** (graph-based vs. executor-based)\n",
    "- **Different tool result handling** in the reasoning loop\n",
    "\n",
    "If you see incomplete responses like *\"was not specified in the search results\"*, you can improve this by:\n",
    "```python\n",
    "# Increase search results for more context\n",
    "search = TavilySearch(max_results=5, api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "\n",
    "# Slightly higher temperature and more tokens\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\", temperature=0.2, max_tokens=2048)\n",
    "\n",
    "# Create the agent\n",
    "tools = [search]\n",
    "graph = create_react_agent(model=llm, tools=tools)\n",
    "```\n",
    "\n",
    "Let's test the basic version first to see how it performs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/NeMo-Agent-Toolkit/examples/notebooks/langgraph_agent.py:21: DeprecationWarning: The 'max_tokens' parameter is deprecated and will be removed in a future version. Please use 'max_completion_tokens' instead.\n",
      "  llm = ChatNVIDIA(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current World Cup holder is the Argentina national team, who defeated the France national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).\n"
     ]
    }
   ],
   "source": [
    "!python langgraph_agent.py \"Who won the last World Cup?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"migration\"></a>\n",
    "# 2.0) Existing Agent Migration\n",
    "\n",
    "<a id=\"migration-part-1\"></a>\n",
    "## 2.1) Migration Part 1: Transforming Your Existing Agent into a Workflow\n",
    "\n",
    "NAT supports users bringing their own agent into the framework. As the primary entrypoint for agent execution is a NAT Workflow. For the first pass at NAT migration we will create a new workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow 'langgraph_agent_workflow' already exists.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat workflow create langgraph_agent_workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created a workflow directory for a new agent, we will continue by migrating the agent's functional code into the new workflow. In the next cell, we have adapted the agent code from the `def existing_agent_main()` into a new method `def langgraph_agent_workflow_function()` which encapsulates the exact same functionality, but is decorated and registered for NAT workflow compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/src/langgraph_agent_workflow/langgraph_agent_workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/src/langgraph_agent_workflow/langgraph_agent_workflow.py\n",
    "import logging\n",
    "\n",
    "from pydantic import Field\n",
    "\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.framework_enum import LLMFrameworkEnum\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LangGraphAgentWorkflowFunctionConfig(FunctionBaseConfig, name=\"langgraph_agent_workflow\"):\n",
    "    pass\n",
    "\n",
    "\n",
    "@register_function(config_type=LangGraphAgentWorkflowFunctionConfig, framework_wrappers=[LLMFrameworkEnum.LANGCHAIN])\n",
    "async def langgraph_agent_workflow_function(_config: LangGraphAgentWorkflowFunctionConfig, _builder: Builder):\n",
    "    import os\n",
    "\n",
    "    from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "    from langchain_tavily import TavilySearch\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "    # Initialize a tool to search the web\n",
    "    search = TavilySearch(\n",
    "        max_results=5,\n",
    "        api_key=os.getenv(\"TAVILY_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Initialize a LLM client\n",
    "    llm = ChatNVIDIA(\n",
    "        model=\"meta/llama-3.3-70b-instruct\",\n",
    "        temperature=0.2,\n",
    "        max_tokens=2048,\n",
    "        api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Create tools list\n",
    "    tools = [search]\n",
    "\n",
    "    # Create a LangGraph ReAct agent using the prebuilt function\n",
    "    # This creates a StateGraph with agent and tool nodes automatically\n",
    "    graph = create_react_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    async def _response_fn(input_message: str) -> str:\n",
    "        response = graph.invoke({\"messages\": [(\"user\", input_message)]})\n",
    "        final_message = response[\"messages\"][-1]\n",
    "        return final_message.content\n",
    "\n",
    "    yield FunctionInfo.from_fn(_response_fn, description=\"A simple LangGraph agent capable of basic internet search\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, this is almost the exact same code as your 'existing' LangGraph agent, but has been refactored to fit within a NAT function registration.\n",
    "\n",
    "The only differences are:\n",
    "1. The definition of a closure function `_response_fn` which captures the instantiated graph and uses that to invoke the agent and return the response\n",
    "2. The use of the `@register_function` decorator\n",
    "3. The async function signature for NAT compatibility\n",
    "\n",
    "**Key Difference from LangChain Migration**:\n",
    "- LangChain agents use `agent_executor.invoke({\"input\": ..., \"chat_history\": []})` and return `response[\"output\"]`\n",
    "- LangGraph agents use `graph.invoke({\"messages\": [(\"user\", ...)]})` and return `response[\"messages\"][-1].content`\n",
    "\n",
    "We can also simplify the workflow configuration to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/configs/config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/configs/config.yml\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can run the new workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:05:01 - INFO     - nat.cli.commands.start:192 - Starting NAT from config file: 'langgraph_agent_workflow/configs/config.yml'\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------\n",
      "Workflow Type: langgraph_agent_workflow\n",
      "Number of Functions: 0\n",
      "Number of Function Groups: 0\n",
      "Number of LLMs: 0\n",
      "Number of Embedders: 0\n",
      "Number of Memory: 0\n",
      "Number of Object Stores: 0\n",
      "Number of Retrievers: 0\n",
      "Number of TTC Strategies: 0\n",
      "Number of Authentication Providers: 0\n",
      "\n",
      "2025-12-04 11:05:16 - INFO     - nat.front_ends.console.console_front_end_plugin:102 - --------------------------------------------------\n",
      "\u001b[32mWorkflow Result:\n",
      "['The current World Cup holder is the Argentina national team, who defeated the France national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).']\u001b[39m\n",
      "--------------------------------------------------\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat run --config_file langgraph_agent_workflow/configs/config.yml --input \"Who won the last World Cup?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"migration-part-2\"></a>\n",
    "## 2.2) Migration Part 2: Making Your Agent Configurable\n",
    "\n",
    "Now that we have a working NAT workflow, let's make it more configurable. We'll parameterize the model name, temperature, and other settings so they can be controlled through the YAML configuration file.\n",
    "\n",
    "This makes the agent more flexible and easier to experiment with different configurations without changing the code.\n",
    "\n",
    "**Improving Response Quality**: If you noticed vague responses in the previous run, we'll address this by:\n",
    "- Increasing `max_search_results` from 2 to 5 (more context)\n",
    "- Raising `temperature` from 0.0 to 0.2 (less conservative reasoning)\n",
    "- Increasing `max_tokens` from 1024 to 2048 (fuller responses)\n",
    "- Enabling `verbose` mode to see the agent's reasoning process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/src/langgraph_agent_workflow/langgraph_agent_workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/src/langgraph_agent_workflow/langgraph_agent_workflow.py\n",
    "import logging\n",
    "\n",
    "\n",
    "from pydantic import Field\n",
    "\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.framework_enum import LLMFrameworkEnum\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LangGraphAgentWorkflowFunctionConfig(FunctionBaseConfig, name=\"langgraph_agent_workflow\"):\n",
    "    \"\"\"Configuration for the LangGraph agent workflow.\"\"\"\n",
    "    model_name: str = Field(\n",
    "        default=\"meta/llama-3.3-70b-instruct\",\n",
    "        description=\"The name of the LLM model to use\"\n",
    "    )\n",
    "    temperature: float = Field(\n",
    "        default=0.2,\n",
    "        description=\"Temperature for LLM sampling\",\n",
    "        ge=0.0,\n",
    "        le=1.0\n",
    "    )\n",
    "    max_tokens: int = Field(\n",
    "        default=2048,\n",
    "        description=\"Maximum number of completion tokens in the response\",\n",
    "        gt=0\n",
    "    )\n",
    "    max_search_results: int = Field(\n",
    "        default=5,\n",
    "        description=\"Maximum number of search results to retrieve\",\n",
    "        gt=0,\n",
    "        le=10\n",
    "    )\n",
    "    verbose: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Enable verbose logging\"\n",
    "    )\n",
    "\n",
    "\n",
    "@register_function(config_type=LangGraphAgentWorkflowFunctionConfig, framework_wrappers=[LLMFrameworkEnum.LANGCHAIN])\n",
    "async def langgraph_agent_workflow_function(config: LangGraphAgentWorkflowFunctionConfig, _builder: Builder):\n",
    "    import os\n",
    "\n",
    "    from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "    from langchain_tavily import TavilySearch\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "    if config.verbose:\n",
    "        logger.info(f\"Initializing LangGraph agent with model: {config.model_name}\")\n",
    "\n",
    "    # Initialize a tool to search the web\n",
    "    search = TavilySearch(\n",
    "        max_results=config.max_search_results,\n",
    "        api_key=os.getenv(\"TAVILY_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Initialize a LLM client with configurable parameters\n",
    "    llm = ChatNVIDIA(\n",
    "        model=config.model_name,\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Create tools list\n",
    "    tools = [search]\n",
    "\n",
    "    # Create a LangGraph ReAct agent\n",
    "    graph = create_react_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    async def _response_fn(input_message: str) -> str:\n",
    "        \"\"\"Execute the LangGraph agent and return the response.\"\"\"\n",
    "        if config.verbose:\n",
    "            logger.info(f\"Processing input: {input_message}\")\n",
    "        \n",
    "        response = graph.invoke({\"messages\": [(\"user\", input_message)]})\n",
    "        final_message = response[\"messages\"][-1]\n",
    "        \n",
    "        if config.verbose:\n",
    "            logger.info(f\"Generated response: {final_message.content}\")\n",
    "        \n",
    "        return final_message.content\n",
    "\n",
    "    yield FunctionInfo.from_fn(_response_fn, description=\"A configurable LangGraph agent capable of internet search\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a more detailed configuration file that takes advantage of these parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/configs/config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/configs/config.yml\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  model_name: meta/llama-3.3-70b-instruct\n",
    "  temperature: 0.2\n",
    "  max_completion_tokens: 2048\n",
    "  max_search_results: 5\n",
    "  verbose: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reinstall the workflow for the changes to take effect:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinstalling workflow 'langgraph_agent_workflow'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow 'langgraph_agent_workflow' reinstalled successfully.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat workflow reinstall langgraph_agent_workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the updated configurable workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:05:22 - INFO     - nat.cli.commands.start:192 - Starting NAT from config file: 'langgraph_agent_workflow/configs/config.yml'\n",
      "2025-12-04 11:05:22 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:53 - Initializing LangGraph agent with model: meta/llama-3.3-70b-instruct\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------\n",
      "Workflow Type: langgraph_agent_workflow\n",
      "Number of Functions: 0\n",
      "Number of Function Groups: 0\n",
      "Number of LLMs: 0\n",
      "Number of Embedders: 0\n",
      "Number of Memory: 0\n",
      "Number of Object Stores: 0\n",
      "Number of Retrievers: 0\n",
      "Number of TTC Strategies: 0\n",
      "Number of Authentication Providers: 0\n",
      "\n",
      "2025-12-04 11:05:22 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:81 - Processing input: Who won the last World Cup?\n",
      "2025-12-04 11:05:24 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:87 - Generated response: The current World Cup holder is the Argentina national team. They defeated the France national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).\n",
      "2025-12-04 11:05:24 - INFO     - nat.front_ends.console.console_front_end_plugin:102 - --------------------------------------------------\n",
      "\u001b[32mWorkflow Result:\n",
      "['The current World Cup holder is the Argentina national team. They defeated the France national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).']\u001b[39m\n",
      "--------------------------------------------------\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat run --config_file langgraph_agent_workflow/configs/config.yml --input \"Who won the last World Cup?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"migration-part-3\"></a>\n",
    "## 2.3) Migration Part 3: Integration with NeMo Agent Toolkit\n",
    "\n",
    "Now let's take it a step further and integrate the LangGraph agent with other NAT components. We can use NAT's built-in LLM management and make the agent use NAT-managed LLMs instead of directly instantiating them.\n",
    "\n",
    "This provides several benefits:\n",
    "- Better observability and tracing\n",
    "- Consistent LLM usage across workflows\n",
    "- Easy model switching through configuration\n",
    "- Integration with NAT's profiling and optimization tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/src/langgraph_agent_workflow/langgraph_agent_workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/src/langgraph_agent_workflow/langgraph_agent_workflow.py\n",
    "import logging\n",
    "\n",
    "from pydantic import Field\n",
    "\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.framework_enum import LLMFrameworkEnum\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.component_ref import LLMRef\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LangGraphAgentWorkflowFunctionConfig(FunctionBaseConfig, name=\"langgraph_agent_workflow\"):\n",
    "    \"\"\"Configuration for the LangGraph agent workflow integrated with NAT.\"\"\"\n",
    "    llm_name: LLMRef = Field(\n",
    "        description=\"Reference to the NAT-managed LLM to use for the agent\"\n",
    "    )\n",
    "    max_search_results: int = Field(\n",
    "        default=2,\n",
    "        description=\"Maximum number of search results to retrieve\",\n",
    "        gt=0,\n",
    "        le=10\n",
    "    )\n",
    "    verbose: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Enable verbose logging\"\n",
    "    )\n",
    "\n",
    "\n",
    "@register_function(config_type=LangGraphAgentWorkflowFunctionConfig, framework_wrappers=[LLMFrameworkEnum.LANGCHAIN])\n",
    "async def langgraph_agent_workflow_function(config: LangGraphAgentWorkflowFunctionConfig, builder: Builder):\n",
    "    import os\n",
    "\n",
    "    from langchain_tavily import TavilySearch\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "    if config.verbose:\n",
    "        logger.info(f\"Initializing LangGraph agent with NAT-managed LLM: {config.llm_name}\")\n",
    "\n",
    "    # Get the LLM from NAT's builder with LangChain wrapper\n",
    "    llm = await builder.get_llm(config.llm_name, wrapper_type=LLMFrameworkEnum.LANGCHAIN)\n",
    "\n",
    "    # Initialize a tool to search the web\n",
    "    search = TavilySearch(\n",
    "        max_results=config.max_search_results,\n",
    "        api_key=os.getenv(\"TAVILY_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Create tools list\n",
    "    tools = [search]\n",
    "\n",
    "    # Create a LangGraph ReAct agent using NAT-managed LLM\n",
    "    graph = create_react_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    async def _response_fn(input_message: str) -> str:\n",
    "        \"\"\"Execute the LangGraph agent and return the response.\"\"\"\n",
    "        if config.verbose:\n",
    "            logger.info(f\"Processing input: {input_message}\")\n",
    "        \n",
    "        response = graph.invoke({\"messages\": [(\"user\", input_message)]})\n",
    "        final_message = response[\"messages\"][-1]\n",
    "        \n",
    "        if config.verbose:\n",
    "            logger.info(f\"Generated response: {final_message.content}\")\n",
    "        \n",
    "        return final_message.content\n",
    "\n",
    "    yield FunctionInfo.from_fn(\n",
    "        _response_fn, \n",
    "        description=\"A NAT-integrated LangGraph agent capable of internet search using Tavily\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now update the configuration to use NAT's LLM management:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/configs/config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/configs/config.yml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.3-70b-instruct\n",
    "    temperature: 0.2\n",
    "    max_tokens: 2048\n",
    "\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  llm_name: nim_llm\n",
    "  max_search_results: 5\n",
    "  verbose: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinstall and test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinstalling workflow 'langgraph_agent_workflow'...\n",
      "Workflow 'langgraph_agent_workflow' reinstalled successfully.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat workflow reinstall langgraph_agent_workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:05:30 - INFO     - nat.cli.commands.start:192 - Starting NAT from config file: 'langgraph_agent_workflow/configs/config.yml'\n",
      "2025-12-04 11:05:30 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:40 - Initializing LangGraph agent with NAT-managed LLM: nim_llm\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------\n",
      "Workflow Type: langgraph_agent_workflow\n",
      "Number of Functions: 0\n",
      "Number of Function Groups: 0\n",
      "Number of LLMs: 1\n",
      "Number of Embedders: 0\n",
      "Number of Memory: 0\n",
      "Number of Object Stores: 0\n",
      "Number of Retrievers: 0\n",
      "Number of TTC Strategies: 0\n",
      "Number of Authentication Providers: 0\n",
      "\n",
      "2025-12-04 11:05:30 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:63 - Processing input: Who won the last World Cup?\n",
      "2025-12-04 11:05:32 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:69 - Generated response: The winner of the last World Cup was Argentina. They defeated France in the 2022 World Cup final with a score of 3-3 (4-2 pens).\n",
      "2025-12-04 11:05:32 - INFO     - nat.front_ends.console.console_front_end_plugin:102 - --------------------------------------------------\n",
      "\u001b[32mWorkflow Result:\n",
      "['The winner of the last World Cup was Argentina. They defeated France in the 2022 World Cup final with a score of 3-3 (4-2 pens).']\u001b[39m\n",
      "--------------------------------------------------\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat run --config_file langgraph_agent_workflow/configs/config.yml --input \"Who won the last World Cup?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"migration-part-4\"></a>\n",
    "## 2.4) Migration Part 4: A Zero-Code Configuration\n",
    "\n",
    "Now that we have a fully integrated LangGraph agent, we can leverage NAT's configuration system to easily switch between different LLMs, adjust parameters, or even compose multiple agents together, all through YAML configuration.\n",
    "\n",
    "For example, you can easily test different models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/configs/config_8b.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/configs/config_8b.yml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.3-70b-instruct\n",
    "    temperature: 0.2\n",
    "    max_tokens: 2048\n",
    "\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  llm_name: nim_llm\n",
    "  max_search_results: 5\n",
    "  verbose: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:05:34 - INFO     - nat.cli.commands.start:192 - Starting NAT from config file: 'langgraph_agent_workflow/configs/config_8b.yml'\n",
      "2025-12-04 11:05:34 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:40 - Initializing LangGraph agent with NAT-managed LLM: nim_llm\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------\n",
      "Workflow Type: langgraph_agent_workflow\n",
      "Number of Functions: 0\n",
      "Number of Function Groups: 0\n",
      "Number of LLMs: 1\n",
      "Number of Embedders: 0\n",
      "Number of Memory: 0\n",
      "Number of Object Stores: 0\n",
      "Number of Retrievers: 0\n",
      "Number of TTC Strategies: 0\n",
      "Number of Authentication Providers: 0\n",
      "\n",
      "2025-12-04 11:05:34 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:63 - Processing input: Who won the last World Cup?\n",
      "2025-12-04 11:05:37 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:69 - Generated response: The current World Cup holder is the Argentina national team. They defeated the French national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).\n",
      "2025-12-04 11:05:37 - INFO     - nat.front_ends.console.console_front_end_plugin:102 - --------------------------------------------------\n",
      "\u001b[32mWorkflow Result:\n",
      "['The current World Cup holder is the Argentina national team. They defeated the French national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).']\u001b[39m\n",
      "--------------------------------------------------\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat run --config_file langgraph_agent_workflow/configs/config_8b.yml --input \"Who won the last World Cup?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also serve your LangGraph agent as an API endpoint:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting: Improving Response Quality\n",
    "\n",
    "### Problem: Vague or Incomplete Responses\n",
    "\n",
    "If you see responses like *\"The winner of the last World Cup was not specified in the search results\"* when the information is available, here's why and how to fix it:\n",
    "\n",
    "#### Common Causes:\n",
    "\n",
    "1. **Too Few Search Results** (max_results=2)\n",
    "   - Only 2 results may miss key information\n",
    "   - **Solution**: Increase to 5-10 results\n",
    "\n",
    "2. **Temperature Too Low** (temperature=0.0)\n",
    "   - Makes the model overly conservative\n",
    "   - **Solution**: Use 0.2-0.3 for better reasoning\n",
    "\n",
    "3. **Token Limits** (max_tokens=1024)\n",
    "   - May cut off the agent's reasoning\n",
    "   - **Solution**: Increase to 2048+\n",
    "\n",
    "4. **Search Tool Limitations**\n",
    "   - Tavily may not always return the best results\n",
    "   - **Solution**: Try different queries or add multiple search tools\n",
    "\n",
    "#### Quick Fix Example:\n",
    "\n",
    "```python\n",
    "# Instead of:\n",
    "search = TavilySearch(max_results=2)\n",
    "llm = ChatNVIDIA(model=\"...\", temperature=0.0, max_tokens=1024)\n",
    "\n",
    "# Use:\n",
    "search = TavilySearch(max_results=5)\n",
    "llm = ChatNVIDIA(model=\"...\", temperature=0.2, max_tokens=2048)\n",
    "```\n",
    "\n",
    "#### In YAML Config:\n",
    "\n",
    "```yaml\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  model_name: meta/llama-3.3-70b-instruct\n",
    "  temperature: 0.2        # Higher for better reasoning\n",
    "  max_tokens: 2048        # More room for complete answers\n",
    "  max_search_results: 5   # More context\n",
    "  verbose: true           # See what's happening\n",
    "```\n",
    "\n",
    "#### Alternative: Add a Custom System Prompt\n",
    "\n",
    "```python\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "system_message = \"\"\"You are a helpful assistant with access to search tools.\n",
    "When you find information in search results, provide direct, complete answers.\n",
    "Always cite the source of your information.\"\"\"\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    messages_modifier=system_message\n",
    ")\n",
    "```\n",
    "\n",
    "This is particularly important for LangGraph agents compared to LangChain's AgentExecutor, as the default system prompts may differ!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to serve the agent (this will run in the background)\n",
    "# !nat serve --config_file langgraph_agent_workflow/configs/config.yml --host 0.0.0.0 --port 8000\n",
    "# Then visit http://localhost:8000/docs for the API documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"next-steps\"></a>\n",
    "# 3) Next Steps\n",
    "\n",
    "Congratulations! You've successfully integrated a LangGraph agent with the NeMo Agent Toolkit. Here are some next steps to explore:\n",
    "\n",
    "## Advanced LangGraph Features\n",
    "\n",
    "1. **Add Custom Tools**: Extend your agent with custom tools beyond web search\n",
    "   ```python\n",
    "   from langchain.tools import tool\n",
    "   \n",
    "   @tool\n",
    "   def custom_calculator(expression: str) -> str:\n",
    "       \"\"\"Evaluate a mathematical expression.\"\"\"\n",
    "       return str(eval(expression))\n",
    "   \n",
    "   tools = [search, custom_calculator]\n",
    "   ```\n",
    "\n",
    "2. **Build Custom Graphs**: Create specialized workflows with custom state\n",
    "   ```python\n",
    "   from langgraph.graph import StateGraph, START, END\n",
    "   from typing import TypedDict, Annotated\n",
    "   from langgraph.graph.message import add_messages\n",
    "   \n",
    "   class AgentState(TypedDict):\n",
    "       messages: Annotated[list, add_messages]\n",
    "       context: str\n",
    "   \n",
    "   graph = StateGraph(AgentState)\n",
    "   graph.add_node(\"planner\", planner_node)\n",
    "   graph.add_node(\"executor\", executor_node)\n",
    "   # Add edges and compile\n",
    "   ```\n",
    "\n",
    "3. **Multi-Agent Systems**: Compose multiple LangGraph agents together\n",
    "4. **Human-in-the-Loop**: Add approval steps in your graph\n",
    "5. **Conditional Routing**: Use conditional edges for complex logic\n",
    "\n",
    "## NAT Integration Features\n",
    "\n",
    "6. **Observability**: Add tracing and monitoring\n",
    "   ```yaml\n",
    "   tracing:\n",
    "     _type: phoenix\n",
    "     endpoint: http://localhost:6006\n",
    "   \n",
    "   workflow:\n",
    "     _type: langgraph_agent_workflow\n",
    "     llm_name: nim_llm\n",
    "     tracing_name: phoenix\n",
    "   ```\n",
    "\n",
    "7. **Memory Integration**: Add persistent memory\n",
    "   ```yaml\n",
    "   memory:\n",
    "     _type: redis\n",
    "     host: localhost\n",
    "     port: 6379\n",
    "   \n",
    "   workflow:\n",
    "     _type: langgraph_agent_workflow\n",
    "     llm_name: nim_llm\n",
    "     memory_name: redis\n",
    "   ```\n",
    "\n",
    "8. **Evaluation**: Use NAT's evaluation tools\n",
    "   ```bash\n",
    "   nat eval --config_file config.yml --dataset eval_dataset.json\n",
    "   ```\n",
    "\n",
    "## Key Differences: LangGraph vs LangChain Agents\n",
    "\n",
    "| Feature | LangChain Agent | LangGraph Agent |\n",
    "|---------|----------------|-----------------|\n",
    "| **Architecture** | AgentExecutor | StateGraph |\n",
    "| **State Management** | Dict-based | Message-based with custom state |\n",
    "| **Input Format** | `{\"input\": ..., \"chat_history\": []}` | `{\"messages\": [(\"user\", ...)]}` |\n",
    "| **Output Format** | `response[\"output\"]` | `response[\"messages\"][-1].content` |\n",
    "| **Flexibility** | Limited | High (custom nodes/edges) |\n",
    "| **Multi-Agent** | Difficult | Native support |\n",
    "| **Human-in-Loop** | Manual implementation | Built-in support |\n",
    "| **Conditional Logic** | Limited | Full control with conditional edges |\n",
    "| **Production Use** | Good for simple cases | Better for complex workflows |\n",
    "\n",
    "## When to Use LangGraph\n",
    "\n",
    "- **Complex agent workflows** with conditional logic\n",
    "- **Multi-agent collaboration** scenarios\n",
    "- When you need **fine-grained control** over execution\n",
    "- **Human-in-the-loop** workflows\n",
    "- Building **production-grade agentic applications**\n",
    "- **State management** across multiple steps\n",
    "- **Parallel execution** of independent tasks\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/)\n",
    "- [NeMo Agent Toolkit Documentation](https://docs.nvidia.com/nemo-agent-toolkit/)\n",
    "- [NVIDIA NIM](https://docs.nvidia.com/nim/)\n",
    "- [LangGraph Examples](https://github.com/langchain-ai/langgraph/tree/main/examples)\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated three progressive levels of LangGraph integration with NAT:\n",
    "\n",
    "1. **V1 (Basic)**: Quick wrap of existing agent with minimal changes\n",
    "2. **V2 (Configurable)**: Parameters exposed in YAML for easy experimentation\n",
    "3. **V3 (Full Integration)**: NAT-managed components for production use\n",
    "\n",
    "Choose the version that best fits your needs, and progressively enhance your integration as your requirements grow!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Adding Observability and Profiling\n",
    "\n",
    "Now that you have a working LangGraph agent, let's add observability and profiling to understand how your agent is performing and where time is being spent.\n",
    "\n",
    "## 4.1) Observability with Phoenix\n",
    "\n",
    "Phoenix provides real-time tracing and visualization of your agent's execution, showing each step, token usage, and latency.\n",
    "\n",
    "### Step 1: Install Phoenix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Phoenix installation complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoenix version: 12.19.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install Phoenix server and NAT integration\n",
    "uv pip show -q \"arize-phoenix\"\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Installing Phoenix server...\"\n",
    "    uv pip install arize-phoenix\n",
    "fi\n",
    "\n",
    "uv pip show -q \"nvidia-nat-phoenix\"\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Installing NAT Phoenix integration...\"\n",
    "    uv pip install \"nvidia-nat[phoenix]\"\n",
    "fi\n",
    "\n",
    "echo \"‚úÖ Phoenix installation complete!\"\n",
    "echo \"Phoenix version: $(python -c 'import phoenix; print(phoenix.__version__)' 2>/dev/null || echo 'installed')\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Start Phoenix Server\n",
    "\n",
    "Phoenix runs as a local server that collects and visualizes traces from your agent.\n",
    "\n",
    "**Important**: Phoenix must be started in a **separate terminal** before running your agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PHOENIX_HOST=0.0.0.0\n"
     ]
    }
   ],
   "source": [
    "%env PHOENIX_HOST=0.0.0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will start the Phoenix server in the background:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "# Phoenix will run on port 6006\n",
    "phoenix serve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Important: Don't Run `!phoenix serve` in Notebook\n",
    "\n",
    "**Why not:**\n",
    "- It's a long-running server that will block the notebook cell\n",
    "- You won't be able to run other cells\n",
    "- The server stops when you interrupt the cell\n",
    "\n",
    "**Instead:** Use a separate terminal (see instructions above) or use the background method below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Phoenix...\n",
      "   Waiting... (1/10)\n",
      "   Waiting... (2/10)\n",
      "   Waiting... (3/10)\n",
      "‚úÖ Phoenix started successfully!\n",
      "üåê Access UI at: http://localhost:6006\n",
      "üìä Process ID: 306342\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Start Phoenix in the background\n",
    "try:\n",
    "    # Kill any existing Phoenix processes\n",
    "    subprocess.run([\"pkill\", \"-f\", \"phoenix\"], stderr=subprocess.DEVNULL)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Start Phoenix in background\n",
    "    phoenix_process = subprocess.Popen(\n",
    "        [\"phoenix\", \"serve\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        cwd=\"/home/ubuntu/NeMo-Agent-Toolkit/examples/notebooks\"\n",
    "    )\n",
    "    \n",
    "    # Wait for Phoenix to start\n",
    "    print(\"Starting Phoenix...\")\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            response = requests.get(\"http://localhost:6006\", timeout=1)\n",
    "            if response.status_code == 200:\n",
    "                print(\"‚úÖ Phoenix started successfully!\")\n",
    "                print(\"üåê Access UI at: http://localhost:6006\")\n",
    "                print(f\"üìä Process ID: {phoenix_process.pid}\")\n",
    "                break\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            print(f\"   Waiting... ({i+1}/10)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Phoenix may not have started. Check manually.\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'phoenix' command not found.\")\n",
    "    print(\"\\nPlease use a separate terminal instead:\")\n",
    "    print(\"  cd /home/ubuntu/NeMo-Agent-Toolkit/examples/notebooks\")\n",
    "    print(\"  python -m phoenix.server.main serve\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To stop Phoenix later:**\n",
    "```python\n",
    "import subprocess\n",
    "subprocess.run([\"pkill\", \"-f\", \"phoenix\"])\n",
    "print(\"Phoenix stopped\")\n",
    "```\n",
    "\n",
    "**To check if Phoenix is running:**\n",
    "```python\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:6006\", timeout=1)\n",
    "    print(f\"‚úÖ Phoenix is running (status: {response.status_code})\")\n",
    "except:\n",
    "    print(\"‚ùå Phoenix is not running\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Simplified Approach (Most Reliable)\n",
    "\n",
    "**Step-by-step Phoenix setup:**\n",
    "\n",
    "1. **Open a NEW terminal window**\n",
    "\n",
    "2. **Navigate to the notebook directory:**\n",
    "   ```bash\n",
    "   cd /home/ubuntu/NeMo-Agent-Toolkit/examples/notebooks\n",
    "   ```\n",
    "\n",
    "3. **Start Phoenix:**\n",
    "   ```bash\n",
    "   phoenix serve\n",
    "   ```\n",
    "   \n",
    "   If that doesn't work, try:\n",
    "   ```bash\n",
    "   python -m phoenix.server.main serve\n",
    "   ```\n",
    "\n",
    "4. **You should see:**\n",
    "   ```\n",
    "   Phoenix server running on http://127.0.0.1:6006\n",
    "   ```\n",
    "\n",
    "5. **Open in browser:** http://localhost:6006\n",
    "\n",
    "6. **Keep that terminal open** - closing it stops Phoenix\n",
    "\n",
    "**That's it!** Phoenix is now ready to collect traces. Proceed to the next cell to configure your agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Update Config with Tracing\n",
    "\n",
    "Now we'll update the workflow configuration to enable Phoenix tracing. We'll append the telemetry section to the existing config:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp langgraph_agent_workflow/configs/config.yml langgraph_agent_workflow/configs/config_with_tracing.yml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to langgraph_agent_workflow/configs/config_with_tracing.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a langgraph_agent_workflow/configs/config_with_tracing.yml\n",
    "\n",
    "general:\n",
    "  telemetry:\n",
    "    logging:\n",
    "      console:\n",
    "        _type: console\n",
    "        level: WARN\n",
    "    tracing:\n",
    "      phoenix:\n",
    "        _type: phoenix\n",
    "        endpoint: http://localhost:6006/v1/traces\n",
    "        project: langgraph_agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Agent with Tracing\n",
    "\n",
    "Now run your agent and traces will be automatically sent to Phoenix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:05:46 - INFO     - nat.cli.commands.start:192 - Starting NAT from config file: 'langgraph_agent_workflow/configs/config_with_tracing.yml'\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------\n",
      "Workflow Type: langgraph_agent_workflow\n",
      "Number of Functions: 0\n",
      "Number of Function Groups: 0\n",
      "Number of LLMs: 1\n",
      "Number of Embedders: 0\n",
      "Number of Memory: 0\n",
      "Number of Object Stores: 0\n",
      "Number of Retrievers: 0\n",
      "Number of TTC Strategies: 0\n",
      "Number of Authentication Providers: 0\n",
      "\n",
      "--------------------------------------------------\n",
      "\u001b[32mWorkflow Result:\n",
      "['The current World Cup holder is the Argentina national team. They defeated the France national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).']\u001b[39m\n",
      "--------------------------------------------------\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat run --config_file langgraph_agent_workflow/configs/config_with_tracing.yml --input \"Who won the last World Cup?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: View Traces in Phoenix UI\n",
    "\n",
    "After running the agent, open the Phoenix UI in your browser to see:\n",
    "\n",
    "1. **Visit**: http://localhost:6006\n",
    "2. **View**:\n",
    "   - Complete trace of agent execution\n",
    "   - Each LLM call with prompts and responses\n",
    "   - Tool calls and their results\n",
    "   - Token usage per step\n",
    "   - Latency breakdown\n",
    "   - Full execution timeline\n",
    "\n",
    "**What You'll See:**\n",
    "- üîç **Spans**: Each operation (LLM call, tool call) as a span\n",
    "- ‚è±Ô∏è **Timing**: How long each step took\n",
    "- üìä **Token Usage**: Input/output tokens per LLM call\n",
    "- üîó **Flow**: Visual graph of agent execution\n",
    "- üìù **Prompts & Responses**: Full text of all interactions\n",
    "\n",
    "This is incredibly valuable for:\n",
    "- Debugging agent behavior\n",
    "- Optimizing performance\n",
    "- Understanding token usage\n",
    "- Identifying bottlenecks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Evaluation Dataset\n",
    "\n",
    "Profiling in NAT works through the `nat eval` command. First, create a simple evaluation dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/data/eval_data.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/data/eval_data.json\n",
    "[\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"question\": \"Who won the last World Cup?\",\n",
    "        \"answer\": \"Argentina won the 2022 FIFA World Cup. They defeated France in the final with a score of 3-3 (4-2 on penalties) in Qatar on December 18, 2022.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\",\n",
    "        \"question\": \"What year did the last World Cup take place?\",\n",
    "        \"answer\": \"The last FIFA World Cup took place in 2022. It was held in Qatar from November 21 to December 18, 2022.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"question\": \"Which country hosted the 2022 World Cup?\",\n",
    "        \"answer\": \"Qatar hosted the 2022 FIFA World Cup. It was the first World Cup held in the Middle East and the first held in November-December rather than the traditional June-July timeframe.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Update Config with Profiler Settings\n",
    "\n",
    "Add profiler configuration to enable detailed performance analysis:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Profiling with NAT's Built-in Profiler\n",
    "\n",
    "NAT includes built-in profiling that measures CPU time, memory usage, and execution time for each component.\n",
    "\n",
    "### Enable Profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: /home/ubuntu/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install profiling dependencies\n",
    "uv pip show -q \"memray\"\n",
    "if [ $? -ne 0 ]; then\n",
    "    uv pip install \"nvidia-nat[profiling]\"\n",
    "else\n",
    "    echo \"Profiling tools are already installed\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_agent_workflow/configs/config_with_profiling.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent_workflow/configs/config_with_profiling.yml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.3-70b-instruct\n",
    "    temperature: 0.2\n",
    "    max_tokens: 2048\n",
    "\n",
    "general:\n",
    "  telemetry:\n",
    "    logging:\n",
    "      console:\n",
    "        _type: console\n",
    "        level: INFO\n",
    "    tracing:\n",
    "      phoenix:\n",
    "        _type: phoenix\n",
    "        endpoint: http://localhost:6006/v1/traces\n",
    "        project: langgraph_agent\n",
    "\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  llm_name: nim_llm\n",
    "  max_search_results: 5\n",
    "  verbose: true\n",
    "\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: ./profile_output\n",
    "    verbose: true\n",
    "    dataset:\n",
    "      _type: json\n",
    "      file_path: ./langgraph_agent_workflow/data/eval_data.json\n",
    "    \n",
    "    profiler:\n",
    "      token_uniqueness_forecast: true\n",
    "      workflow_runtime_forecast: true\n",
    "      compute_llm_metrics: true\n",
    "      csv_exclude_io_text: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Config with Profiling\n",
    "\n",
    "Create a complete config file that includes workflow, tracing, and evaluation with profiling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation with Profiling\n",
    "\n",
    "Run the evaluation which will automatically generate profiling data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:06:08 - INFO     - nat.eval.evaluate:446 - Starting evaluation run with config file: langgraph_agent_workflow/configs/config_with_profiling.yml\n",
      "2025-12-04 11:06:08 - INFO     - phoenix.config:1750 - üìã Ensuring phoenix working directory: /home/ubuntu/.phoenix\n",
      "2025-12-04 11:06:08 - INFO     - phoenix.inferences.inferences:112 - Dataset: phoenix_inferences_3e86ea26-8d55-4f3e-8539-cc3842854e18 initialized\n",
      "2025-12-04 11:06:10 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:40 - Initializing LangGraph agent with NAT-managed LLM: nim_llm\n",
      "Running workflow:   0%|                                   | 0/3 [00:00<?, ?it/s]2025-12-04 11:06:10 - INFO     - nat.observability.exporter_manager:269 - Started exporter 'phoenix'\n",
      "2025-12-04 11:06:10 - INFO     - nat.observability.exporter_manager:269 - Started exporter 'phoenix'\n",
      "2025-12-04 11:06:10 - INFO     - nat.observability.exporter_manager:269 - Started exporter 'phoenix'\n",
      "2025-12-04 11:06:10 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:63 - Processing input: Who won the last World Cup?\n",
      "2025-12-04 11:06:27 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:69 - Generated response: The current World Cup holder is the Argentina national team. They defeated the France national team in the 2022 World Cup final in Qatar with a score of 3-3 (4-2 pens).\n",
      "2025-12-04 11:06:27 - INFO     - nat.observability.exporter.base_exporter:283 - Event stream completed. No more events will arrive.\n",
      "2025-12-04 11:06:27 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:63 - Processing input: What year did the last World Cup take place?\n",
      "2025-12-04 11:06:44 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:69 - Generated response: The last World Cup took place in 2022.\n",
      "2025-12-04 11:06:44 - INFO     - nat.observability.exporter.base_exporter:283 - Event stream completed. No more events will arrive.\n",
      "2025-12-04 11:06:44 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:63 - Processing input: Which country hosted the 2022 World Cup?\n",
      "2025-12-04 11:07:01 - INFO     - langgraph_agent_workflow.langgraph_agent_workflow:69 - Generated response: The country that hosted the 2022 World Cup was Qatar.\n",
      "2025-12-04 11:07:01 - INFO     - nat.observability.exporter.base_exporter:283 - Event stream completed. No more events will arrive.\n",
      "2025-12-04 11:07:01 - INFO     - nat.observability.exporter_manager:275 - Stopped exporter 'phoenix'\n",
      "2025-12-04 11:07:01 - INFO     - nat.observability.exporter_manager:275 - Stopped exporter 'phoenix'\n",
      "2025-12-04 11:07:01 - INFO     - nat.observability.exporter_manager:275 - Stopped exporter 'phoenix'\n",
      "Running workflow: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:51<00:00, 17.06s/it]\n",
      "2025-12-04 11:07:01 - WARNING  - nat.eval.evaluate:377 - All evaluators were empty or invalid.\n",
      "2025-12-04 11:07:01 - INFO     - nat.eval.evaluate:426 - Waiting for export tasks from 1 local exporters (timeout: 60s)\n",
      "2025-12-04 11:07:01 - INFO     - nat.eval.evaluate:431 - Export tasks completed for exporter: phoenix\n",
      "2025-12-04 11:07:01 - INFO     - nat.eval.evaluate:435 - All local export task waiting completed\n",
      "2025-12-04 11:07:01 - INFO     - nat.profiler.profile_runner:127 - Wrote combined data to: profile_output/all_requests_profiler_traces.json\n",
      "2025-12-04 11:07:01 - INFO     - nat.profiler.profile_runner:146 - Wrote merged standardized DataFrame to profile_output/standardized_data_all.csv\n",
      "2025-12-04 11:07:01 - INFO     - nat.profiler.profile_runner:200 - Wrote inference optimization results to: profile_output/inference_optimization.json\n",
      "2025-12-04 11:07:01 - INFO     - nat.eval.evaluate:335 - Workflow output written to profile_output/workflow_output.json\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!mkdir -p langgraph_agent_workflow/data\n",
    "!nat eval --config_file langgraph_agent_workflow/configs/config_with_profiling.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: View Profiling Results\n",
    "\n",
    "After the evaluation completes, check the profiling output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Profiling Output Files ===\n",
      "total 296K\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 212K Dec  4 11:07 all_requests_profiler_traces.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 1.7K Dec  4 11:07 inference_optimization.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 6.3K Dec  4 11:07 standardized_data_all.csv\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  70K Dec  4 11:07 workflow_output.json\n",
      "\n",
      "Expected files:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - all_requests_profiler_traces.json  (Raw LLM traces)\n",
      "  - inference_optimization.json        (Performance metrics)\n",
      "  - standardized_data_all.csv          (Token usage data)\n"
     ]
    }
   ],
   "source": [
    "!echo \"=== Profiling Output Files ===\"\n",
    "!ls -lh ./profile_output/ 2>/dev/null || echo \"Run the evaluation cell above first\"\n",
    "!echo \"\"\n",
    "!echo \"Expected files:\"\n",
    "!echo \"  - all_requests_profiler_traces.json  (Raw LLM traces)\"\n",
    "!echo \"  - inference_optimization.json        (Performance metrics)\"\n",
    "!echo \"  - standardized_data_all.csv          (Token usage data)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Results\n",
    "\n",
    "After running evaluation with profiling, you'll find several files in `./profile_output/`:\n",
    "\n",
    "#### Core Output Files\n",
    "\n",
    "**1. `all_requests_profiler_traces.json`**\n",
    "- Raw traces of all LLM interactions\n",
    "- Tool input and output data\n",
    "- Runtime measurements for each component\n",
    "- Complete execution metadata\n",
    "\n",
    "**2. `inference_optimization.json`**\n",
    "- Workflow performance metrics with confidence intervals\n",
    "- 90%, 95%, and 99% confidence intervals for latency\n",
    "- Throughput statistics\n",
    "- Workflow runtime predictions\n",
    "- Token usage forecasts\n",
    "\n",
    "**3. `standardized_data_all.csv`**\n",
    "- Standardized usage data in CSV format\n",
    "- Prompt tokens and completion tokens per request\n",
    "- LLM input/output text\n",
    "- Framework information (LangGraph)\n",
    "- Timing and metadata for each evaluation question\n",
    "\n",
    "#### Advanced Analysis Files (if enabled in config)\n",
    "\n",
    "**4. Analysis Reports**\n",
    "- **Bottleneck analysis**: Identifies slowest components in your workflow\n",
    "- **Concurrency analysis**: Shows parallel execution opportunities\n",
    "- **Token uniqueness forecast**: Predicts token efficiency for future queries\n",
    "\n",
    "### Key Metrics to Watch\n",
    "\n",
    "| Metric | Description | Where to Find |\n",
    "|--------|-------------|---------------|\n",
    "| **Total Latency** | End-to-end response time | `inference_optimization.json` |\n",
    "| **Token Usage** | Input/output tokens per request | `standardized_data_all.csv` |\n",
    "| **LLM Time** | Time spent in LLM calls | `all_requests_profiler_traces.json` |\n",
    "| **Tool Time** | Time spent in search/tools | Trace JSON, individual tool spans |\n",
    "| **Cost Estimate** | Approximate API costs | Calculate from token counts |\n",
    "\n",
    "### Example: Viewing Results\n",
    "\n",
    "```python\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# View optimization metrics\n",
    "with open('./profile_output/inference_optimization.json') as f:\n",
    "    metrics = json.load(f)\n",
    "    print(f\"Average latency: {metrics.get('avg_latency', 'N/A')}s\")\n",
    "\n",
    "# View detailed usage data\n",
    "df = pd.read_csv('./profile_output/standardized_data_all.csv')\n",
    "print(f\"Total tokens used: {df['prompt_tokens'].sum() + df['completion_tokens'].sum()}\")\n",
    "print(f\"Average response time: {df['execution_time'].mean():.2f}s\")\n",
    "```\n",
    "\n",
    "### Optimization Tips\n",
    "\n",
    "1. **High LLM Time**: \n",
    "   - Use smaller models (8B instead of 70B)\n",
    "   - Reduce max_tokens\n",
    "   - Cache common queries\n",
    "\n",
    "2. **High Tool Time**:\n",
    "   - Reduce max_search_results\n",
    "   - Use faster search APIs\n",
    "   - Implement tool result caching\n",
    "\n",
    "3. **High Memory**:\n",
    "   - Reduce conversation history\n",
    "   - Clear unused variables\n",
    "   - Use streaming responses\n",
    "\n",
    "4. **High Token Usage**:\n",
    "   - Optimize prompts\n",
    "   - Reduce search result content\n",
    "   - Use more focused tool descriptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) Combined: Observability + Profiling Best Practices\n",
    "\n",
    "### Recommended Development Workflow\n",
    "\n",
    "1. **Development** (Local):\n",
    "   - Enable verbose logging\n",
    "   - Use Phoenix for tracing\n",
    "   - Profile periodically\n",
    "\n",
    "2. **Testing** (Pre-Production):\n",
    "   - Enable profiling for all test runs\n",
    "   - Monitor memory usage\n",
    "   - Track token consumption\n",
    "\n",
    "3. **Production**:\n",
    "   - Lightweight tracing (sample rate)\n",
    "   - Continuous performance monitoring\n",
    "   - Alert on anomalies\n",
    "\n",
    "### Example Combined Configuration\n",
    "\n",
    "```yaml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.3-70b-instruct\n",
    "    temperature: 0.2\n",
    "    max_tokens: 2048\n",
    "\n",
    "tracing:\n",
    "  phoenix_tracer:\n",
    "    _type: phoenix\n",
    "    endpoint: http://localhost:6006\n",
    "    sample_rate: 1.0  # 100% in dev, lower in prod\n",
    "\n",
    "profiling:\n",
    "  enabled: true\n",
    "  output_dir: ./profiling_results\n",
    "  profile_memory: true\n",
    "  profile_cpu: true\n",
    "\n",
    "logging:\n",
    "  level: INFO\n",
    "  format: json\n",
    "  output: ./logs/agent.log\n",
    "\n",
    "workflow:\n",
    "  _type: langgraph_agent_workflow\n",
    "  llm_name: nim_llm\n",
    "  max_search_results: 5\n",
    "  verbose: true\n",
    "  tracing_name: phoenix_tracer\n",
    "```\n",
    "\n",
    "### Quick Debugging Commands\n",
    "\n",
    "```bash\n",
    "# View traces in real-time\n",
    "open http://localhost:6006\n",
    "\n",
    "# Check profiling results\n",
    "ls -lh ./profiling_results/\n",
    "\n",
    "# View memory profile\n",
    "memray flamegraph ./profiling_results/memory.bin\n",
    "\n",
    "# Analyze CPU profile\n",
    "python -m pstats ./profiling_results/cpu_profile.prof\n",
    "\n",
    "# Check logs\n",
    "tail -f ./logs/agent.log | jq '.'\n",
    "```\n",
    "\n",
    "### Benefits of Observability + Profiling\n",
    "\n",
    "‚úÖ **Faster Debugging**: See exactly what the agent is doing  \n",
    "‚úÖ **Performance Optimization**: Identify bottlenecks quickly  \n",
    "‚úÖ **Cost Management**: Track token usage and API calls  \n",
    "‚úÖ **Quality Assurance**: Verify agent behavior  \n",
    "‚úÖ **Production Readiness**: Monitor health in real-time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
